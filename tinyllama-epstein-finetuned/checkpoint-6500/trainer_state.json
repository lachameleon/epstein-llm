{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.004314122008763,
  "eval_steps": 500,
  "global_step": 6500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0793965859468043,
      "grad_norm": 2.2148830890655518,
      "learning_rate": 9.30379746835443e-05,
      "loss": 7.8113330078125,
      "step": 50
    },
    {
      "epoch": 0.1587931718936086,
      "grad_norm": 0.9728765487670898,
      "learning_rate": 0.00018797468354430377,
      "loss": 7.297769775390625,
      "step": 100
    },
    {
      "epoch": 0.23818975784041285,
      "grad_norm": 1.2643409967422485,
      "learning_rate": 0.00028291139240506325,
      "loss": 7.137250366210938,
      "step": 150
    },
    {
      "epoch": 0.3175863437872172,
      "grad_norm": 1.254591464996338,
      "learning_rate": 0.00029588903743315507,
      "loss": 7.042655029296875,
      "step": 200
    },
    {
      "epoch": 0.39698292973402144,
      "grad_norm": 1.2084978818893433,
      "learning_rate": 0.00029087566844919786,
      "loss": 6.99340576171875,
      "step": 250
    },
    {
      "epoch": 0.4763795156808257,
      "grad_norm": 1.229156732559204,
      "learning_rate": 0.00028586229946524065,
      "loss": 6.945512084960938,
      "step": 300
    },
    {
      "epoch": 0.55577610162763,
      "grad_norm": 1.3278619050979614,
      "learning_rate": 0.0002808489304812834,
      "loss": 6.87389892578125,
      "step": 350
    },
    {
      "epoch": 0.6351726875744343,
      "grad_norm": 2.09529709815979,
      "learning_rate": 0.00027583556149732616,
      "loss": 6.845098266601562,
      "step": 400
    },
    {
      "epoch": 0.7145692735212386,
      "grad_norm": 1.162826418876648,
      "learning_rate": 0.00027082219251336895,
      "loss": 6.836253051757812,
      "step": 450
    },
    {
      "epoch": 0.7939658594680429,
      "grad_norm": 1.7856515645980835,
      "learning_rate": 0.00026580882352941174,
      "loss": 6.790972290039062,
      "step": 500
    },
    {
      "epoch": 0.8733624454148472,
      "grad_norm": 1.0448341369628906,
      "learning_rate": 0.0002607954545454545,
      "loss": 6.783919677734375,
      "step": 550
    },
    {
      "epoch": 0.9527590313616514,
      "grad_norm": 1.2746527194976807,
      "learning_rate": 0.0002557820855614973,
      "loss": 6.74372802734375,
      "step": 600
    },
    {
      "epoch": 1.0317586343787217,
      "grad_norm": 0.9762573838233948,
      "learning_rate": 0.0002507687165775401,
      "loss": 6.80138427734375,
      "step": 650
    },
    {
      "epoch": 1.111155220325526,
      "grad_norm": 1.0766390562057495,
      "learning_rate": 0.0002457553475935829,
      "loss": 6.713706665039062,
      "step": 700
    },
    {
      "epoch": 1.1905518062723304,
      "grad_norm": 1.5280882120132446,
      "learning_rate": 0.00024074197860962564,
      "loss": 6.6685986328125,
      "step": 750
    },
    {
      "epoch": 1.2699483922191346,
      "grad_norm": 1.7530767917633057,
      "learning_rate": 0.00023572860962566843,
      "loss": 6.725615234375,
      "step": 800
    },
    {
      "epoch": 1.3493449781659388,
      "grad_norm": 1.0622074604034424,
      "learning_rate": 0.00023071524064171122,
      "loss": 6.703662719726562,
      "step": 850
    },
    {
      "epoch": 1.4287415641127432,
      "grad_norm": 1.0529228448867798,
      "learning_rate": 0.00022570187165775398,
      "loss": 6.637001342773438,
      "step": 900
    },
    {
      "epoch": 1.5081381500595474,
      "grad_norm": 1.7144129276275635,
      "learning_rate": 0.00022068850267379676,
      "loss": 6.686971435546875,
      "step": 950
    },
    {
      "epoch": 1.5875347360063516,
      "grad_norm": 1.3873200416564941,
      "learning_rate": 0.00021567513368983955,
      "loss": 6.7185498046875,
      "step": 1000
    },
    {
      "epoch": 3.333465660976578,
      "grad_norm": 0.853554368019104,
      "learning_rate": 0.0002106617647058823,
      "loss": 6.64152587890625,
      "step": 1050
    },
    {
      "epoch": 3.4922588328701867,
      "grad_norm": 0.9382859468460083,
      "learning_rate": 0.00020564839572192512,
      "loss": 6.613890991210938,
      "step": 1100
    },
    {
      "epoch": 3.651052004763795,
      "grad_norm": 1.0767990350723267,
      "learning_rate": 0.0002006350267379679,
      "loss": 6.681602783203125,
      "step": 1150
    },
    {
      "epoch": 3.8098451766574035,
      "grad_norm": 0.7878811955451965,
      "learning_rate": 0.0001956216577540107,
      "loss": 6.6588067626953125,
      "step": 1200
    },
    {
      "epoch": 3.9686383485510124,
      "grad_norm": 0.9671106338500977,
      "learning_rate": 0.00019060828877005346,
      "loss": 6.631044311523437,
      "step": 1250
    },
    {
      "epoch": 4.127034537514887,
      "grad_norm": 1.093563437461853,
      "learning_rate": 0.00018559491978609624,
      "loss": 6.614917602539062,
      "step": 1300
    },
    {
      "epoch": 4.285827709408496,
      "grad_norm": 1.0501011610031128,
      "learning_rate": 0.00018058155080213903,
      "loss": 6.613800048828125,
      "step": 1350
    },
    {
      "epoch": 4.444620881302104,
      "grad_norm": 0.8489177227020264,
      "learning_rate": 0.0001755681818181818,
      "loss": 6.58972900390625,
      "step": 1400
    },
    {
      "epoch": 4.603414053195713,
      "grad_norm": 0.917880117893219,
      "learning_rate": 0.00017055481283422458,
      "loss": 6.585474853515625,
      "step": 1450
    },
    {
      "epoch": 4.7622072250893215,
      "grad_norm": 0.9473081827163696,
      "learning_rate": 0.00016554144385026736,
      "loss": 6.581447143554687,
      "step": 1500
    },
    {
      "epoch": 4.9210003969829295,
      "grad_norm": 0.8658047914505005,
      "learning_rate": 0.00016052807486631012,
      "loss": 6.612150268554688,
      "step": 1550
    },
    {
      "epoch": 5.079396585946804,
      "grad_norm": 1.1906020641326904,
      "learning_rate": 0.00015551470588235294,
      "loss": 6.564219970703125,
      "step": 1600
    },
    {
      "epoch": 5.238189757840413,
      "grad_norm": 1.3664405345916748,
      "learning_rate": 0.00015050133689839572,
      "loss": 6.512674560546875,
      "step": 1650
    },
    {
      "epoch": 5.396982929734022,
      "grad_norm": 1.0390194654464722,
      "learning_rate": 0.00014548796791443848,
      "loss": 6.553822021484375,
      "step": 1700
    },
    {
      "epoch": 5.55577610162763,
      "grad_norm": 1.3073437213897705,
      "learning_rate": 0.00014047459893048127,
      "loss": 6.517490844726563,
      "step": 1750
    },
    {
      "epoch": 5.714569273521239,
      "grad_norm": 0.9219369888305664,
      "learning_rate": 0.00013546122994652406,
      "loss": 6.58367431640625,
      "step": 1800
    },
    {
      "epoch": 5.873362445414847,
      "grad_norm": 1.0838003158569336,
      "learning_rate": 0.00013044786096256682,
      "loss": 6.594120483398438,
      "step": 1850
    },
    {
      "epoch": 6.031758634378722,
      "grad_norm": 1.234140396118164,
      "learning_rate": 0.0001254344919786096,
      "loss": 6.547242431640625,
      "step": 1900
    },
    {
      "epoch": 6.19055180627233,
      "grad_norm": 1.191261887550354,
      "learning_rate": 0.0001204211229946524,
      "loss": 6.522792358398437,
      "step": 1950
    },
    {
      "epoch": 6.349344978165939,
      "grad_norm": 0.9209516048431396,
      "learning_rate": 0.00011540775401069518,
      "loss": 6.50240234375,
      "step": 2000
    },
    {
      "epoch": 6.508138150059548,
      "grad_norm": 0.9971117973327637,
      "learning_rate": 0.00011039438502673796,
      "loss": 6.520127563476563,
      "step": 2050
    },
    {
      "epoch": 6.666931321953156,
      "grad_norm": 1.3375729322433472,
      "learning_rate": 0.00010538101604278074,
      "loss": 6.545632934570312,
      "step": 2100
    },
    {
      "epoch": 6.8257244938467645,
      "grad_norm": 1.25674569606781,
      "learning_rate": 0.00010036764705882351,
      "loss": 6.511025390625,
      "step": 2150
    },
    {
      "epoch": 6.984517665740373,
      "grad_norm": 1.202010989189148,
      "learning_rate": 9.535427807486631e-05,
      "loss": 6.54970703125,
      "step": 2200
    },
    {
      "epoch": 7.142913854704248,
      "grad_norm": 1.0170197486877441,
      "learning_rate": 9.034090909090908e-05,
      "loss": 6.49872314453125,
      "step": 2250
    },
    {
      "epoch": 7.301707026597857,
      "grad_norm": 1.41182541847229,
      "learning_rate": 8.532754010695186e-05,
      "loss": 6.501796264648437,
      "step": 2300
    },
    {
      "epoch": 7.460500198491465,
      "grad_norm": 1.102301836013794,
      "learning_rate": 8.031417112299464e-05,
      "loss": 6.467081909179687,
      "step": 2350
    },
    {
      "epoch": 7.619293370385074,
      "grad_norm": 1.1654448509216309,
      "learning_rate": 7.530080213903742e-05,
      "loss": 6.533358154296875,
      "step": 2400
    },
    {
      "epoch": 7.7780865422786825,
      "grad_norm": 1.173929214477539,
      "learning_rate": 7.02874331550802e-05,
      "loss": 6.4889593505859375,
      "step": 2450
    },
    {
      "epoch": 7.93687971417229,
      "grad_norm": 1.337776780128479,
      "learning_rate": 6.527406417112299e-05,
      "loss": 6.51190185546875,
      "step": 2500
    },
    {
      "epoch": 8.095275903136166,
      "grad_norm": 1.1807750463485718,
      "learning_rate": 6.026069518716577e-05,
      "loss": 6.485402221679688,
      "step": 2550
    },
    {
      "epoch": 8.254069075029774,
      "grad_norm": 1.1915977001190186,
      "learning_rate": 5.524732620320855e-05,
      "loss": 6.467850952148438,
      "step": 2600
    },
    {
      "epoch": 8.412862246923382,
      "grad_norm": 1.218049168586731,
      "learning_rate": 5.023395721925133e-05,
      "loss": 6.457567749023437,
      "step": 2650
    },
    {
      "epoch": 8.571655418816992,
      "grad_norm": 1.2053401470184326,
      "learning_rate": 4.522058823529411e-05,
      "loss": 6.475690307617188,
      "step": 2700
    },
    {
      "epoch": 8.7304485907106,
      "grad_norm": 1.2721405029296875,
      "learning_rate": 4.02072192513369e-05,
      "loss": 6.49098876953125,
      "step": 2750
    },
    {
      "epoch": 8.889241762604208,
      "grad_norm": 1.372667670249939,
      "learning_rate": 3.519385026737968e-05,
      "loss": 6.515907592773438,
      "step": 2800
    },
    {
      "epoch": 9.047637951568083,
      "grad_norm": 1.1836516857147217,
      "learning_rate": 3.0180481283422457e-05,
      "loss": 6.459091796875,
      "step": 2850
    },
    {
      "epoch": 9.206431123461691,
      "grad_norm": 1.0455775260925293,
      "learning_rate": 2.516711229946524e-05,
      "loss": 6.509469604492187,
      "step": 2900
    },
    {
      "epoch": 9.365224295355299,
      "grad_norm": 0.9802221655845642,
      "learning_rate": 2.015374331550802e-05,
      "loss": 6.487177734375,
      "step": 2950
    },
    {
      "epoch": 9.524017467248909,
      "grad_norm": 1.1465363502502441,
      "learning_rate": 1.51403743315508e-05,
      "loss": 6.4558477783203125,
      "step": 3000
    },
    {
      "epoch": 9.682810639142517,
      "grad_norm": 1.2992907762527466,
      "learning_rate": 1.0127005347593584e-05,
      "loss": 6.46743408203125,
      "step": 3050
    },
    {
      "epoch": 9.841603811036126,
      "grad_norm": 1.1768245697021484,
      "learning_rate": 5.113636363636363e-06,
      "loss": 6.503694458007812,
      "step": 3100
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.6623185873031616,
      "learning_rate": 1.0026737967914437e-07,
      "loss": 6.4508294677734375,
      "step": 3150
    },
    {
      "epoch": 3.4486686889113582,
      "grad_norm": 1.3255927562713623,
      "learning_rate": 0.00020693058076225044,
      "loss": 6.678828735351562,
      "step": 3200
    },
    {
      "epoch": 3.5025952140208965,
      "grad_norm": 1.5001264810562134,
      "learning_rate": 0.0002052291288566243,
      "loss": 6.362440795898437,
      "step": 3250
    },
    {
      "epoch": 3.5565217391304347,
      "grad_norm": 1.4332040548324585,
      "learning_rate": 0.00020352767695099816,
      "loss": 6.201503295898437,
      "step": 3300
    },
    {
      "epoch": 3.610448264239973,
      "grad_norm": 1.6032978296279907,
      "learning_rate": 0.00020182622504537204,
      "loss": 5.952816772460937,
      "step": 3350
    },
    {
      "epoch": 3.6643747893495116,
      "grad_norm": 1.8964604139328003,
      "learning_rate": 0.00020012477313974592,
      "loss": 5.938712158203125,
      "step": 3400
    },
    {
      "epoch": 3.7183013144590493,
      "grad_norm": 2.1631789207458496,
      "learning_rate": 0.00019842332123411977,
      "loss": 5.83675537109375,
      "step": 3450
    },
    {
      "epoch": 3.772227839568588,
      "grad_norm": 1.6837414503097534,
      "learning_rate": 0.00019672186932849362,
      "loss": 5.813786010742188,
      "step": 3500
    },
    {
      "epoch": 3.826154364678126,
      "grad_norm": 1.5224777460098267,
      "learning_rate": 0.0001950204174228675,
      "loss": 5.780405883789062,
      "step": 3550
    },
    {
      "epoch": 3.8800808897876644,
      "grad_norm": 1.7659581899642944,
      "learning_rate": 0.00019331896551724135,
      "loss": 5.786303100585937,
      "step": 3600
    },
    {
      "epoch": 3.9340074148972026,
      "grad_norm": 1.7945307493209839,
      "learning_rate": 0.00019161751361161523,
      "loss": 5.7790155029296875,
      "step": 3650
    },
    {
      "epoch": 3.987933940006741,
      "grad_norm": 2.022951602935791,
      "learning_rate": 0.0001899160617059891,
      "loss": 5.6872607421875,
      "step": 3700
    },
    {
      "epoch": 4.040984159083249,
      "grad_norm": 1.6694854497909546,
      "learning_rate": 0.00018821460980036298,
      "loss": 5.63830322265625,
      "step": 3750
    },
    {
      "epoch": 4.094910684192787,
      "grad_norm": 1.2847477197647095,
      "learning_rate": 0.00018651315789473683,
      "loss": 5.703458251953125,
      "step": 3800
    },
    {
      "epoch": 4.148837209302325,
      "grad_norm": 1.893333911895752,
      "learning_rate": 0.00018481170598911068,
      "loss": 5.646932373046875,
      "step": 3850
    },
    {
      "epoch": 4.202763734411864,
      "grad_norm": 1.7309744358062744,
      "learning_rate": 0.00018311025408348456,
      "loss": 5.581823120117187,
      "step": 3900
    },
    {
      "epoch": 4.2566902595214025,
      "grad_norm": 1.4352378845214844,
      "learning_rate": 0.0001814088021778584,
      "loss": 5.5933380126953125,
      "step": 3950
    },
    {
      "epoch": 4.31061678463094,
      "grad_norm": 1.7641806602478027,
      "learning_rate": 0.00017970735027223229,
      "loss": 5.573262329101563,
      "step": 4000
    },
    {
      "epoch": 4.364543309740479,
      "grad_norm": 1.5145530700683594,
      "learning_rate": 0.00017800589836660616,
      "loss": 5.597614135742187,
      "step": 4050
    },
    {
      "epoch": 4.418469834850017,
      "grad_norm": 1.508603811264038,
      "learning_rate": 0.00017630444646098004,
      "loss": 5.554592895507812,
      "step": 4100
    },
    {
      "epoch": 4.472396359959555,
      "grad_norm": 1.577239990234375,
      "learning_rate": 0.0001746029945553539,
      "loss": 5.530490112304688,
      "step": 4150
    },
    {
      "epoch": 4.526322885069093,
      "grad_norm": 1.8325724601745605,
      "learning_rate": 0.00017290154264972774,
      "loss": 5.574821166992187,
      "step": 4200
    },
    {
      "epoch": 4.580249410178632,
      "grad_norm": 1.269425868988037,
      "learning_rate": 0.00017120009074410162,
      "loss": 5.504465942382812,
      "step": 4250
    },
    {
      "epoch": 4.6341759352881695,
      "grad_norm": 1.379846453666687,
      "learning_rate": 0.00016949863883847547,
      "loss": 5.420396118164063,
      "step": 4300
    },
    {
      "epoch": 4.688102460397708,
      "grad_norm": 1.8585338592529297,
      "learning_rate": 0.00016779718693284935,
      "loss": 5.524961547851563,
      "step": 4350
    },
    {
      "epoch": 4.742028985507247,
      "grad_norm": 3.289742946624756,
      "learning_rate": 0.00016609573502722322,
      "loss": 5.472234497070312,
      "step": 4400
    },
    {
      "epoch": 4.795955510616785,
      "grad_norm": 1.404090404510498,
      "learning_rate": 0.0001643942831215971,
      "loss": 5.473297119140625,
      "step": 4450
    },
    {
      "epoch": 4.849882035726323,
      "grad_norm": 1.9496352672576904,
      "learning_rate": 0.00016269283121597095,
      "loss": 5.5706494140625,
      "step": 4500
    },
    {
      "epoch": 4.903808560835861,
      "grad_norm": 1.9354428052902222,
      "learning_rate": 0.0001609913793103448,
      "loss": 5.57994384765625,
      "step": 4550
    },
    {
      "epoch": 4.9577350859454,
      "grad_norm": 1.8690009117126465,
      "learning_rate": 0.00015928992740471868,
      "loss": 5.419383544921875,
      "step": 4600
    },
    {
      "epoch": 5.0107853050219076,
      "grad_norm": 1.4180387258529663,
      "learning_rate": 0.00015758847549909253,
      "loss": 5.4498486328125,
      "step": 4650
    },
    {
      "epoch": 5.064711830131446,
      "grad_norm": 1.610098958015442,
      "learning_rate": 0.0001558870235934664,
      "loss": 5.465941772460938,
      "step": 4700
    },
    {
      "epoch": 5.118638355240984,
      "grad_norm": 1.7865155935287476,
      "learning_rate": 0.00015418557168784028,
      "loss": 5.541659545898438,
      "step": 4750
    },
    {
      "epoch": 5.172564880350523,
      "grad_norm": 2.0256800651550293,
      "learning_rate": 0.00015248411978221416,
      "loss": 5.498038940429687,
      "step": 4800
    },
    {
      "epoch": 5.22649140546006,
      "grad_norm": 1.6140700578689575,
      "learning_rate": 0.000150782667876588,
      "loss": 5.277249145507812,
      "step": 4850
    },
    {
      "epoch": 5.280417930569599,
      "grad_norm": 1.563006043434143,
      "learning_rate": 0.00014908121597096186,
      "loss": 5.45326416015625,
      "step": 4900
    },
    {
      "epoch": 5.334344455679137,
      "grad_norm": 1.4737682342529297,
      "learning_rate": 0.00014737976406533574,
      "loss": 5.457156982421875,
      "step": 4950
    },
    {
      "epoch": 5.3882709807886755,
      "grad_norm": 1.2967262268066406,
      "learning_rate": 0.0001456783121597096,
      "loss": 5.44334716796875,
      "step": 5000
    },
    {
      "epoch": 5.442197505898213,
      "grad_norm": 1.7811615467071533,
      "learning_rate": 0.00014397686025408347,
      "loss": 5.31092041015625,
      "step": 5050
    },
    {
      "epoch": 5.496124031007752,
      "grad_norm": 1.7361527681350708,
      "learning_rate": 0.00014227540834845735,
      "loss": 5.361223754882812,
      "step": 5100
    },
    {
      "epoch": 5.55005055611729,
      "grad_norm": 1.8893311023712158,
      "learning_rate": 0.0001405739564428312,
      "loss": 5.389174194335937,
      "step": 5150
    },
    {
      "epoch": 5.603977081226828,
      "grad_norm": 1.7266653776168823,
      "learning_rate": 0.00013887250453720507,
      "loss": 5.4669482421875,
      "step": 5200
    },
    {
      "epoch": 5.657903606336367,
      "grad_norm": 1.8184897899627686,
      "learning_rate": 0.00013717105263157895,
      "loss": 5.310223999023438,
      "step": 5250
    },
    {
      "epoch": 5.711830131445905,
      "grad_norm": 1.7094815969467163,
      "learning_rate": 0.0001354696007259528,
      "loss": 5.368212890625,
      "step": 5300
    },
    {
      "epoch": 5.7657566565554434,
      "grad_norm": 1.421846628189087,
      "learning_rate": 0.00013376814882032665,
      "loss": 5.44444580078125,
      "step": 5350
    },
    {
      "epoch": 5.819683181664981,
      "grad_norm": 1.6439321041107178,
      "learning_rate": 0.00013206669691470053,
      "loss": 5.386240234375,
      "step": 5400
    },
    {
      "epoch": 5.87360970677452,
      "grad_norm": 1.686049222946167,
      "learning_rate": 0.0001303652450090744,
      "loss": 5.435254516601563,
      "step": 5450
    },
    {
      "epoch": 5.927536231884058,
      "grad_norm": 1.415423035621643,
      "learning_rate": 0.00012866379310344826,
      "loss": 5.287724609375,
      "step": 5500
    },
    {
      "epoch": 5.981462756993596,
      "grad_norm": 1.4401246309280396,
      "learning_rate": 0.00012696234119782213,
      "loss": 5.372681274414062,
      "step": 5550
    },
    {
      "epoch": 6.034512976070104,
      "grad_norm": 1.4526783227920532,
      "learning_rate": 0.000125260889292196,
      "loss": 5.447670288085938,
      "step": 5600
    },
    {
      "epoch": 6.088439501179643,
      "grad_norm": 1.6110109090805054,
      "learning_rate": 0.00012355943738656986,
      "loss": 5.38937255859375,
      "step": 5650
    },
    {
      "epoch": 6.142366026289181,
      "grad_norm": 1.5265527963638306,
      "learning_rate": 0.00012185798548094373,
      "loss": 5.3854833984375,
      "step": 5700
    },
    {
      "epoch": 6.196292551398719,
      "grad_norm": 1.4659273624420166,
      "learning_rate": 0.0001201565335753176,
      "loss": 5.266890258789062,
      "step": 5750
    },
    {
      "epoch": 6.250219076508257,
      "grad_norm": 1.4860494136810303,
      "learning_rate": 0.00011845508166969145,
      "loss": 5.349188232421875,
      "step": 5800
    },
    {
      "epoch": 6.304145601617796,
      "grad_norm": 2.1289901733398438,
      "learning_rate": 0.00011675362976406532,
      "loss": 5.34481689453125,
      "step": 5850
    },
    {
      "epoch": 6.358072126727334,
      "grad_norm": 1.7096790075302124,
      "learning_rate": 0.0001150521778584392,
      "loss": 5.328059692382812,
      "step": 5900
    },
    {
      "epoch": 6.411998651836872,
      "grad_norm": 1.6911592483520508,
      "learning_rate": 0.00011335072595281306,
      "loss": 5.31366455078125,
      "step": 5950
    },
    {
      "epoch": 6.465925176946411,
      "grad_norm": 1.6586209535598755,
      "learning_rate": 0.00011164927404718692,
      "loss": 5.320150146484375,
      "step": 6000
    },
    {
      "epoch": 6.5198517020559486,
      "grad_norm": 1.5947469472885132,
      "learning_rate": 0.00010994782214156079,
      "loss": 5.232992553710938,
      "step": 6050
    },
    {
      "epoch": 6.573778227165487,
      "grad_norm": 1.568669319152832,
      "learning_rate": 0.00010824637023593466,
      "loss": 5.334613037109375,
      "step": 6100
    },
    {
      "epoch": 6.627704752275025,
      "grad_norm": 2.015399932861328,
      "learning_rate": 0.00010654491833030853,
      "loss": 5.31802490234375,
      "step": 6150
    },
    {
      "epoch": 6.681631277384564,
      "grad_norm": 1.5974867343902588,
      "learning_rate": 0.00010484346642468238,
      "loss": 5.4080859375,
      "step": 6200
    },
    {
      "epoch": 6.735557802494101,
      "grad_norm": 1.4083012342453003,
      "learning_rate": 0.00010314201451905625,
      "loss": 5.391422119140625,
      "step": 6250
    },
    {
      "epoch": 6.78948432760364,
      "grad_norm": 1.629675269126892,
      "learning_rate": 0.00010144056261343012,
      "loss": 5.3071832275390625,
      "step": 6300
    },
    {
      "epoch": 6.843410852713179,
      "grad_norm": 1.7902175188064575,
      "learning_rate": 9.973911070780398e-05,
      "loss": 5.387142944335937,
      "step": 6350
    },
    {
      "epoch": 6.8973373778227165,
      "grad_norm": 1.5673683881759644,
      "learning_rate": 9.803765880217785e-05,
      "loss": 5.280414428710937,
      "step": 6400
    },
    {
      "epoch": 6.951263902932255,
      "grad_norm": 1.3991472721099854,
      "learning_rate": 9.633620689655172e-05,
      "loss": 5.232937622070312,
      "step": 6450
    },
    {
      "epoch": 7.004314122008763,
      "grad_norm": 1.7625316381454468,
      "learning_rate": 9.463475499092559e-05,
      "loss": 5.324850463867188,
      "step": 6500
    }
  ],
  "logging_steps": 50,
  "max_steps": 9280,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 835636545454080.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
